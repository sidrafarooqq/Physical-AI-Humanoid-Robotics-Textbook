"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[44],{8453:(n,e,r)=>{r.d(e,{R:()=>s,x:()=>o});var t=r(6540);const a={},i=t.createContext(a);function s(n){const e=t.useContext(i);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),t.createElement(i.Provider,{value:e},n.children)}},9785:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Module-4-Vision-Language-Action-VLA/chapter2","title":"Chapter 2: Cognitive Planning - LLMs Translating Language to Action","description":"2.1 Large Language Models in Robotics","source":"@site/docs/Module-4-Vision-Language-Action-VLA/chapter2.md","sourceDirName":"Module-4-Vision-Language-Action-VLA","slug":"/Module-4-Vision-Language-Action-VLA/chapter2","permalink":"/docs/Module-4-Vision-Language-Action-VLA/chapter2","draft":false,"unlisted":false,"editUrl":"https://github.com/<your-username>/<your-repo>/tree/main/docs/Module-4-Vision-Language-Action-VLA/chapter2.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: Voice-to-Action - Speech Recognition and Command Processing","permalink":"/docs/Module-4-Vision-Language-Action-VLA/chapter1"},"next":{"title":"Chapter 3: From Planning to Execution - ROS 2 Action Integration","permalink":"/docs/Module-4-Vision-Language-Action-VLA/chapter3"}}');var a=r(4848),i=r(8453);const s={},o="Chapter 2: Cognitive Planning - LLMs Translating Language to Action",l={},c=[{value:"2.1 Large Language Models in Robotics",id:"21-large-language-models-in-robotics",level:3},{value:"2.2 Cognitive Planning Architecture",id:"22-cognitive-planning-architecture",level:3},{value:"2.2.1 System Overview",id:"221-system-overview",level:4},{value:"2.2.2 Prompt Engineering for Task Planning",id:"222-prompt-engineering-for-task-planning",level:4},{value:"2.2.3 Hierarchical Planning with Subtasks",id:"223-hierarchical-planning-with-subtasks",level:4},{value:"2.3 Context-Aware Planning",id:"23-context-aware-planning",level:3},{value:"2.3.1 Incorporating Environmental Context",id:"231-incorporating-environmental-context",level:4},{value:"2.4 Error Handling and Replanning",id:"24-error-handling-and-replanning",level:3},{value:"2.4.1 Failure Detection and Recovery",id:"241-failure-detection-and-recovery",level:4}];function p(n){const e={code:"code",h1:"h1",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"chapter-2-cognitive-planning---llms-translating-language-to-action",children:"Chapter 2: Cognitive Planning - LLMs Translating Language to Action"})}),"\n",(0,a.jsx)(e.h3,{id:"21-large-language-models-in-robotics",children:"2.1 Large Language Models in Robotics"}),"\n",(0,a.jsx)(e.p,{children:"Large Language Models (LLMs) like GPT-4, Claude, and Llama excel at understanding context, reasoning about tasks, and generating structured outputs. This capability transforms how robots plan complex tasks."}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"LLM Advantages for Robotics:"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Common Sense Reasoning"}),": Understands real-world constraints"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Task Decomposition"}),": Breaks complex goals into steps"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Error Recovery"}),": Generates backup plans"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Multi-step Planning"}),": Coordinates multiple robot actions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Natural Language Interface"}),": Accepts human descriptions"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"22-cognitive-planning-architecture",children:"2.2 Cognitive Planning Architecture"}),"\n",(0,a.jsx)(e.h4,{id:"221-system-overview",children:"2.2.1 System Overview"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:'User: "Clean the room"\r\n     \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Voice Recognition       \u2502 \u2192 "clean the room"\r\n\u2502 (Whisper)               \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n     \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Intent Recognition      \u2502 \u2192 Intent: "clean"\r\n\u2502 (Command Parser)        \u2502    Objects: "room"\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n     \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Cognitive Planning      \u2502 \u2192 Plan: [\r\n\u2502 (LLM)                   \u2502    1. Map room\r\n\u2502                         \u2502    2. Identify dirt\r\n\u2502                         \u2502    3. Pick up trash\r\n\u2502                         \u2502    4. Return to start\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n     \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Task Execution          \u2502 \u2192 Execute ROS 2\r\n\u2502 (ROS 2 Action Servers)  \u2502    actions\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n'})}),"\n",(0,a.jsx)(e.h4,{id:"222-prompt-engineering-for-task-planning",children:"2.2.2 Prompt Engineering for Task Planning"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# LLM-based Task Planner\r\nimport openai\r\nfrom typing import List, Dict\r\n\r\nclass CognitivePlanner:\r\n    """Use LLM for task planning"""\r\n    \r\n    def __init__(self, model="gpt-4"):\r\n        self.model = model\r\n        self.api_key = os.getenv(\'OPENAI_API_KEY\')\r\n        openai.api_key = self.api_key\r\n        \r\n        # System prompt defining robot capabilities\r\n        self.system_prompt = """\r\nYou are an AI planning assistant for a humanoid robot. \r\nThe robot has the following capabilities:\r\n- navigate(destination: str) - Move to a named location\r\n- pick_up(object: str) - Grab an object\r\n- place(object: str, location: str) - Place object at location\r\n- look_for(object: str) - Search for an object\r\n- open(target: str) - Open door/drawer\r\n- close(target: str) - Close door/drawer\r\n- move_arm(target_pose: str) - Move arm to target\r\n- gripper_open() / gripper_close() - Control gripper\r\n- wait(seconds: float) - Wait/pause\r\n\r\nCurrent environment:\r\n- Locations: living_room, kitchen, bedroom, bathroom, hallway\r\n- Objects: trash, dishes, books, clothes, dust\r\n- Sensors: RGB camera, depth camera, LIDAR, IMU\r\n\r\nWhen given a task:\r\n1. Break it down into atomic actions\r\n2. Use only available capabilities\r\n3. Consider dependencies between actions\r\n4. Include safety checks\r\n5. Return plan as JSON array\r\n\r\nAlways respond with valid JSON only, no additional text.\r\n"""\r\n    \r\n    def plan_task(self, task_description: str) -> List[Dict]:\r\n        """\r\n        Generate task plan from natural language description\r\n        \r\n        Args:\r\n            task_description: What user wants robot to do\r\n            \r\n        Returns:\r\n            List of atomic actions with parameters\r\n        """\r\n        user_prompt = f"""\r\nPlan the following task for the humanoid robot:\r\n"{task_description}"\r\n\r\nReturn the plan as a JSON array where each element is:\r\n{{\r\n    "action": "action_name",\r\n    "parameters": {{"param1": "value1", "param2": "value2"}},\r\n    "description": "What this step does"\r\n}}\r\n\r\nExample for "Pick up the book":\r\n[\r\n    {{"action": "look_for", "parameters": {{"object": "book"}}, "description": "Find the book"}},\r\n    {{"action": "navigate", "parameters": {{"destination": "book location"}}, "description": "Move to book"}},\r\n    {{"action": "pick_up", "parameters": {{"object": "book"}}, "description": "Grasp the book"}}\r\n]\r\n"""\r\n        \r\n        try:\r\n            response = openai.ChatCompletion.create(\r\n                model=self.model,\r\n                messages=[\r\n                    {"role": "system", "content": self.system_prompt},\r\n                    {"role": "user", "content": user_prompt}\r\n                ],\r\n                temperature=0.3,  # Lower for more deterministic planning\r\n                max_tokens=2000\r\n            )\r\n            \r\n            # Extract plan from response\r\n            plan_text = response[\'choices\'][0][\'message\'][\'content\']\r\n            \r\n            # Parse JSON\r\n            import json\r\n            plan = json.loads(plan_text)\r\n            \r\n            return plan\r\n            \r\n        except json.JSONDecodeError as e:\r\n            print(f"Failed to parse LLM response as JSON: {e}")\r\n            return []\r\n        except openai.error.OpenAIError as e:\r\n            print(f"OpenAI API error: {e}")\r\n            return []\r\n    \r\n    def validate_plan(self, plan: List[Dict]) -> Tuple[bool, str]:\r\n        """\r\n        Validate that plan is executable\r\n        \r\n        Args:\r\n            plan: Generated task plan\r\n            \r\n        Returns:\r\n            (is_valid, reason)\r\n        """\r\n        valid_actions = {\r\n            \'navigate\', \'pick_up\', \'place\', \'look_for\', \r\n            \'open\', \'close\', \'move_arm\', \'gripper_open\', \r\n            \'gripper_close\', \'wait\'\r\n        }\r\n        \r\n        if not isinstance(plan, list):\r\n            return False, "Plan must be a list of actions"\r\n        \r\n        if len(plan) == 0:\r\n            return False, "Plan is empty"\r\n        \r\n        if len(plan) > 50:\r\n            return False, "Plan too long (max 50 steps)"\r\n        \r\n        for i, step in enumerate(plan):\r\n            # Check action exists\r\n            if \'action\' not in step:\r\n                return False, f"Step {i}: Missing \'action\' field"\r\n            \r\n            action = step[\'action\']\r\n            if action not in valid_actions:\r\n                return False, f"Step {i}: Unknown action \'{action}\'"\r\n            \r\n            # Check parameters exist\r\n            if \'parameters\' not in step:\r\n                return False, f"Step {i}: Missing \'parameters\' field"\r\n        \r\n        return True, "Plan is valid"\n'})}),"\n",(0,a.jsx)(e.h4,{id:"223-hierarchical-planning-with-subtasks",children:"2.2.3 Hierarchical Planning with Subtasks"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class HierarchicalPlanner:\r\n    """Support multi-level task planning"""\r\n    \r\n    def __init__(self):\r\n        self.base_planner = CognitivePlanner()\r\n        self.subtask_cache = {}\r\n    \r\n    def decompose_complex_task(self, task: str, depth: int = 0) -> List[Dict]:\r\n        """\r\n        Recursively decompose complex tasks\r\n        \r\n        Args:\r\n            task: High-level task description\r\n            depth: Current recursion depth\r\n            \r\n        Returns:\r\n            Fully decomposed action plan\r\n        """\r\n        if depth > 3:  # Prevent infinite recursion\r\n            return []\r\n        \r\n        # Get initial plan\r\n        plan = self.base_planner.plan_task(task)\r\n        \r\n        # Check if plan contains abstract actions\r\n        abstract_actions = []\r\n        for i, step in enumerate(plan):\r\n            if self._is_abstract(step[\'action\']):\r\n                abstract_actions.append((i, step))\r\n        \r\n        # If no abstract actions, return plan\r\n        if not abstract_actions:\r\n            return plan\r\n        \r\n        # Decompose abstract actions\r\n        expanded_plan = []\r\n        for i, (index, step) in enumerate(abstract_actions):\r\n            # Add all steps before this abstract action\r\n            if i == 0:\r\n                expanded_plan.extend(plan[:index])\r\n            else:\r\n                prev_index = abstract_actions[i-1][0]\r\n                expanded_plan.extend(plan[prev_index+1:index])\r\n            \r\n            # Recursively decompose abstract action\r\n            subtask = f"{step[\'action\']}({\', \'.join(step[\'parameters\'].values())})"\r\n            subtask_plan = self.decompose_complex_task(subtask, depth + 1)\r\n            expanded_plan.extend(subtask_plan)\r\n        \r\n        # Add remaining steps\r\n        if abstract_actions:\r\n            last_index = abstract_actions[-1][0]\r\n            expanded_plan.extend(plan[last_index+1:])\r\n        \r\n        return expanded_plan\r\n    \r\n    def _is_abstract(self, action: str) -> bool:\r\n        """Check if action is abstract (needs decomposition)"""\r\n        abstract_actions = {\'clean\', \'tidy\', \'organize\', \'prepare\'}\r\n        return action in abstract_actions\n'})}),"\n",(0,a.jsx)(e.h3,{id:"23-context-aware-planning",children:"2.3 Context-Aware Planning"}),"\n",(0,a.jsx)(e.h4,{id:"231-incorporating-environmental-context",children:"2.3.1 Incorporating Environmental Context"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class ContextualPlanner:\r\n    """Plan with awareness of environment state"""\r\n    \r\n    def __init__(self):\r\n        self.planner = CognitivePlanner()\r\n        self.world_state = {}\r\n        self.robot_state = {}\r\n    \r\n    def update_world_state(self, perception_data: Dict):\r\n        """Update understanding of environment"""\r\n        self.world_state.update(perception_data)\r\n    \r\n    def update_robot_state(self, state_data: Dict):\r\n        """Update robot status (battery, position, etc)"""\r\n        self.robot_state.update(state_data)\r\n    \r\n    def plan_with_context(self, task: str) -> List[Dict]:\r\n        """\r\n        Generate plan considering current state\r\n        \r\n        Args:\r\n            task: Task description\r\n            \r\n        Returns:\r\n            Context-aware plan\r\n        """\r\n        # Build context prompt\r\n        context = f"""\r\nCurrent Robot State:\r\n- Position: {self.robot_state.get(\'position\', \'unknown\')}\r\n- Battery: {self.robot_state.get(\'battery\', \'unknown\')}%\r\n- Gripper: {\'open\' if self.robot_state.get(\'gripper_open\') else \'closed\'}\r\n- Current task: {self.robot_state.get(\'current_task\', \'idle\')}\r\n\r\nWorld State:\r\n- Detected objects: {self.world_state.get(\'objects\', [])}\r\n- Obstacles: {self.world_state.get(\'obstacles\', [])}\r\n- Door status: {self.world_state.get(\'doors\', {})}\r\n"""\r\n        \r\n        full_prompt = f"{context}\\n\\nTask: {task}"\r\n        \r\n        return self.planner.plan_task(full_prompt)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"24-error-handling-and-replanning",children:"2.4 Error Handling and Replanning"}),"\n",(0,a.jsx)(e.h4,{id:"241-failure-detection-and-recovery",children:"2.4.1 Failure Detection and Recovery"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class AdaptivePlanner:\r\n    \"\"\"Handle plan failures and adapt\"\"\"\r\n    \r\n    def __init__(self):\r\n        self.planner = CognitivePlanner()\r\n        self.current_plan = []\r\n        self.execution_history = []\r\n    \r\n    def execute_with_recovery(self, plan: List[Dict], \r\n                             max_retries: int = 3) -> Tuple[bool, str]:\r\n        \"\"\"\r\n        Execute plan with automatic recovery on failure\r\n        \r\n        Args:\r\n            plan: Action plan to execute\r\n            max_retries: Times to retry on failure\r\n            \r\n        Returns:\r\n            (success, final_status)\r\n        \"\"\"\r\n        self.current_plan = plan\r\n        \r\n        for step_idx, step in enumerate(plan):\r\n            success = False\r\n            \r\n            for retry in range(max_retries):\r\n                try:\r\n                    # Execute step\r\n                    result = self._execute_action(step)\r\n                    \r\n                    # Log execution\r\n                    self.execution_history.append({\r\n                        'step': step_idx,\r\n                        'action': step['action'],\r\n                        'status': 'success',\r\n                        'timestamp': time.time()\r\n                    })\r\n                    \r\n                    success = True\r\n                    break\r\n                    \r\n                except ActionFailureException as e:\r\n                    self.execution_history.append({\r\n                        'step': step_idx,\r\n                        'action': step['action'],\r\n                        'status': 'failed',\r\n                        'error': str(e),\r\n                        'retry': retry,\r\n                        'timestamp': time.time()\r\n                    })\r\n                    \r\n                    if retry < max_retries - 1:\r\n                        # Wait before retry\r\n                        time.sleep(2 ** retry)  # Exponential backoff\r\n            \r\n            if not success:\r\n                # Generate recovery plan\r\n                recovery_plan = self._generate_recovery_plan(\r\n                    step, \r\n                    self.execution_history\r\n                )\r\n                \r\n                if recovery_plan:\r\n                    # Execute recovery plan\r\n                    _, recovery_status = self.execute_with_recovery(\r\n                        recovery_plan, \r\n                        max_retries=1\r\n                    )\r\n                else:\r\n                    return False, f\"Failed at step {step_idx}: {step['action']}\"\r\n        \r\n        return True, \"Plan executed successfully\"\r\n    \r\n    def _generate_recovery_plan(self, failed_step: Dict, \r\n                               history: List[Dict]) -> List[Dict]:\r\n        \"\"\"Generate recovery plan for failed action\"\"\"\r\n        \r\n        recovery_prompt = f\"\"\"\r\nThe robot failed to execute the following action:\r\n{failed_step['action']}({', '.join(failed_step['parameters'].values())})\r\n\r\nRecent execution history:\r\n{history[-5:]}\r\n\r\nGenerate an alternative plan to recover from this failure.\r\nConsider different approaches or intermediate steps needed.\r\n\"\"\"\r\n        \r\n        recovery_plan = self.planner.plan_task(recovery_prompt)\r\n        return recovery_plan\n"})}),"\n",(0,a.jsx)(e.hr,{})]})}function d(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(p,{...n})}):p(n)}}}]);