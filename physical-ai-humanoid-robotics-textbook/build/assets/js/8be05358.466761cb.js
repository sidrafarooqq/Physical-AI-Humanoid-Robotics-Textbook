"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[285],{7218:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Module-2-The-Digital-Twin-Gazebo-&-Unity/chapter3","title":"Chapter 3: Sensor Simulation and Data Acquisition","description":"3.1 LiDAR (Light Detection and Ranging)","source":"@site/docs/Module-2-The-Digital-Twin-Gazebo-&-Unity/chapter3.md","sourceDirName":"Module-2-The-Digital-Twin-Gazebo-&-Unity","slug":"/Module-2-The-Digital-Twin-Gazebo-&-Unity/chapter3","permalink":"/docs/Module-2-The-Digital-Twin-Gazebo-&-Unity/chapter3","draft":false,"unlisted":false,"editUrl":"https://github.com/<your-username>/<your-repo>/tree/main/docs/Module-2-The-Digital-Twin-Gazebo-&-Unity/chapter3.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 2: Humanoid Robot Simulation in Gazebo","permalink":"/docs/Module-2-The-Digital-Twin-Gazebo-&-Unity/chapter2"},"next":{"title":"Chapter 4: High-Fidelity Rendering and Unity Integration","permalink":"/docs/Module-2-The-Digital-Twin-Gazebo-&-Unity/chapter4"}}');var s=i(4848),r=i(8453);const a={},o="Chapter 3: Sensor Simulation and Data Acquisition",l={},c=[{value:"3.1 LiDAR (Light Detection and Ranging)",id:"31-lidar-light-detection-and-ranging",level:3},{value:"3.2 Depth Cameras",id:"32-depth-cameras",level:3},{value:"3.3 Inertial Measurement Units (IMUs)",id:"33-inertial-measurement-units-imus",level:3},{value:"3.4 Sensor Fusion and Data Processing",id:"34-sensor-fusion-and-data-processing",level:3}];function d(n){const e={code:"code",h1:"h1",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"chapter-3-sensor-simulation-and-data-acquisition",children:"Chapter 3: Sensor Simulation and Data Acquisition"})}),"\n",(0,s.jsx)(e.h3,{id:"31-lidar-light-detection-and-ranging",children:"3.1 LiDAR (Light Detection and Ranging)"}),"\n",(0,s.jsx)(e.p,{children:"LiDAR sensors measure distance by emitting laser beams and analyzing reflections. Gazebo simulates LiDAR with the following capabilities:"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Key Parameters:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Horizontal Resolution"}),": Number of beams in horizontal plane (typically 720-1440)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vertical Resolution"}),": Number of vertical scans (typically 16-64)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Range"}),": Minimum and maximum detection distance (5-200m)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Update Rate"}),": Frequency of scan generation (5-20 Hz)"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Simulated Output:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"Point Cloud Data:\r\n(x, y, z, intensity)\r\nRange from [0.0m, 30.0m]\n"})}),"\n",(0,s.jsx)(e.h3,{id:"32-depth-cameras",children:"3.2 Depth Cameras"}),"\n",(0,s.jsx)(e.p,{children:"Depth cameras provide 3D spatial information using structured light or time-of-flight technology. In Gazebo simulation:"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Parameters:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Resolution"}),": Typical 640\xd7480 or 1280\xd7720 pixels"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Field of View"}),": Usually 60-90 degrees"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Depth Range"}),": 0.3-10 meters"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Frame Rate"}),": 30-60 fps"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Data Acquisition:"}),"\r\nDepth cameras output a depth image where each pixel represents distance from the camera, allowing for obstacle detection and environment mapping."]}),"\n",(0,s.jsx)(e.h3,{id:"33-inertial-measurement-units-imus",children:"3.3 Inertial Measurement Units (IMUs)"}),"\n",(0,s.jsx)(e.p,{children:"IMUs measure acceleration and angular velocity, providing crucial feedback for robot balance and orientation."}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Simulated Measurements:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Linear Acceleration"}),": x, y, z components (m/s\xb2)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Angular Velocity"}),": Roll, pitch, yaw rates (rad/s)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Orientation"}),": Quaternion or Euler angles"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Noise Models"}),": Gaussian noise added to simulate real sensors"]}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# IMU data structure in simulation\r\nimu_data = {\r\n    'linear_acceleration': [0.0, 0.0, 9.81],  # m/s\xb2\r\n    'angular_velocity': [0.0, 0.0, 0.0],      # rad/s\r\n    'orientation': [0.0, 0.0, 0.0, 1.0],      # quaternion\r\n    'timestamp': 1234567890.123               # seconds\r\n}\n"})}),"\n",(0,s.jsx)(e.h3,{id:"34-sensor-fusion-and-data-processing",children:"3.4 Sensor Fusion and Data Processing"}),"\n",(0,s.jsx)(e.p,{children:"Multiple sensors provide complementary information. Sensor fusion techniques combine data from LiDAR, depth cameras, and IMUs for robust environment perception:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Data Synchronization"}),": Aligning sensor data timestamps"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Filtering"}),": Removing noise and outliers"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Registration"}),": Aligning point clouds from multiple sensors"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Feature Extraction"}),": Identifying objects and surfaces in the environment"]}),"\n"]}),"\n",(0,s.jsx)(e.hr,{})]})}function h(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>o});var t=i(6540);const s={},r=t.createContext(s);function a(n){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:a(n.components),t.createElement(r.Provider,{value:e},n.children)}}}]);