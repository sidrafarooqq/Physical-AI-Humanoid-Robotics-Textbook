"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[260],{8023:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"Module-4-Vision-Language-Action-VLA/chapter1","title":"Chapter 1: Voice-to-Action - Speech Recognition and Command Processing","description":"1.1 Introduction to Voice-Enabled Robotics","source":"@site/docs/Module-4-Vision-Language-Action-VLA/chapter1.md","sourceDirName":"Module-4-Vision-Language-Action-VLA","slug":"/Module-4-Vision-Language-Action-VLA/chapter1","permalink":"/docs/Module-4-Vision-Language-Action-VLA/chapter1","draft":false,"unlisted":false,"editUrl":"https://github.com/<your-username>/<your-repo>/tree/main/docs/Module-4-Vision-Language-Action-VLA/chapter1.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Integration and End-to-End AI-Robot System","permalink":"/docs/Module-3-The-AI-Robot-Brain-NVIDIA-Isaac\u2122/chapter4"},"next":{"title":"Chapter 2: Cognitive Planning - LLMs Translating Language to Action","permalink":"/docs/Module-4-Vision-Language-Action-VLA/chapter2"}}');var i=r(4848),o=r(8453);const a={},s="Chapter 1: Voice-to-Action - Speech Recognition and Command Processing",c={},d=[{value:"1.1 Introduction to Voice-Enabled Robotics",id:"11-introduction-to-voice-enabled-robotics",level:3},{value:"1.2 OpenAI Whisper: Speech-to-Text Foundation",id:"12-openai-whisper-speech-to-text-foundation",level:3},{value:"1.2.1 Architecture and Capabilities",id:"121-architecture-and-capabilities",level:4},{value:"1.2.2 Acoustic Modeling",id:"122-acoustic-modeling",level:4},{value:"1.2.3 Implementation on Robots",id:"123-implementation-on-robots",level:4},{value:"1.2.4 Handling Edge Cases and Robustness",id:"124-handling-edge-cases-and-robustness",level:4},{value:"1.3 Command Parsing and Validation",id:"13-command-parsing-and-validation",level:3},{value:"1.3.1 Intent Recognition",id:"131-intent-recognition",level:4},{value:"1.3.2 Confidence Scoring",id:"132-confidence-scoring",level:4}];function l(n){const e={code:"code",h1:"h1",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"chapter-1-voice-to-action---speech-recognition-and-command-processing",children:"Chapter 1: Voice-to-Action - Speech Recognition and Command Processing"})}),"\n",(0,i.jsx)(e.h3,{id:"11-introduction-to-voice-enabled-robotics",children:"1.1 Introduction to Voice-Enabled Robotics"}),"\n",(0,i.jsx)(e.p,{children:"Voice interfaces represent a critical frontier in human-robot interaction by enabling intuitive, natural communication. This chapter explores how robots can listen, understand, and act on spoken commands using OpenAI's Whisper model and language processing frameworks."}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Why Voice Commands Matter:"})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Accessibility"}),": Non-technical users can control robots naturally"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Real-time Interaction"}),": Faster than manual control or GUI interfaces"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Hands-free Operation"}),": Essential for collaborative robotics scenarios"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Cognitive Load Reduction"}),": Users think in natural language, not robot commands"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"12-openai-whisper-speech-to-text-foundation",children:"1.2 OpenAI Whisper: Speech-to-Text Foundation"}),"\n",(0,i.jsx)(e.h4,{id:"121-architecture-and-capabilities",children:"1.2.1 Architecture and Capabilities"}),"\n",(0,i.jsx)(e.p,{children:"OpenAI Whisper is a robust, multilingual automatic speech recognition (ASR) system trained on 680,000 hours of multilingual and multitask supervised data collected from the web."}),"\n",(0,i.jsx)(e.p,{children:(0,i.jsx)(e.strong,{children:"Model Variants:"})}),"\n",(0,i.jsxs)(e.table,{children:[(0,i.jsx)(e.thead,{children:(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.th,{children:"Model"}),(0,i.jsx)(e.th,{children:"Parameters"}),(0,i.jsx)(e.th,{children:"English-Only Speed"}),(0,i.jsx)(e.th,{children:"Multilingual Speed"}),(0,i.jsx)(e.th,{children:"Typical Use Case"})]})}),(0,i.jsxs)(e.tbody,{children:[(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Tiny"})}),(0,i.jsx)(e.td,{children:"39M"}),(0,i.jsx)(e.td,{children:"~32x faster"}),(0,i.jsx)(e.td,{children:"~16x faster"}),(0,i.jsx)(e.td,{children:"Edge devices, Jetson"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Base"})}),(0,i.jsx)(e.td,{children:"74M"}),(0,i.jsx)(e.td,{children:"~16x faster"}),(0,i.jsx)(e.td,{children:"~8x faster"}),(0,i.jsx)(e.td,{children:"Real-time, onboard"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Small"})}),(0,i.jsx)(e.td,{children:"244M"}),(0,i.jsx)(e.td,{children:"~6x faster"}),(0,i.jsx)(e.td,{children:"~3x faster"}),(0,i.jsx)(e.td,{children:"High accuracy, fast"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Medium"})}),(0,i.jsx)(e.td,{children:"769M"}),(0,i.jsx)(e.td,{children:"~2x faster"}),(0,i.jsx)(e.td,{children:"~1.6x faster"}),(0,i.jsx)(e.td,{children:"Production standard"})]}),(0,i.jsxs)(e.tr,{children:[(0,i.jsx)(e.td,{children:(0,i.jsx)(e.strong,{children:"Large"})}),(0,i.jsx)(e.td,{children:"1.5B"}),(0,i.jsx)(e.td,{children:"Real-time"}),(0,i.jsx)(e.td,{children:"Real-time"}),(0,i.jsx)(e.td,{children:"Maximum accuracy"})]})]})]}),"\n",(0,i.jsx)(e.h4,{id:"122-acoustic-modeling",children:"1.2.2 Acoustic Modeling"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Audio Waveform\r\n     \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Log-Mel Spectrogram \u2502 (80 mel-frequency bins)\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n     \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Encoder (Transformer) \u2502 \u2192 Audio embeddings\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n     \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502 Decoder (Transformer) \u2502 \u2192 Text tokens\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n     \u2193\r\nTranscribed Text\n"})}),"\n",(0,i.jsx)(e.h4,{id:"123-implementation-on-robots",children:"1.2.3 Implementation on Robots"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# OpenAI Whisper Integration on Humanoid Robot\r\nimport openai\r\nimport numpy as np\r\nimport pyaudio\r\nimport rclpy\r\nfrom rclpy.node import Node\r\n\r\nclass VoiceCommandNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'voice_command_node\')\r\n        \r\n        # Initialize Whisper\r\n        self.model = openai.api_resources.Model.retrieve("whisper-1")\r\n        \r\n        # Audio recording parameters\r\n        self.CHUNK = 2048\r\n        self.FORMAT = pyaudio.paFloat32\r\n        self.CHANNELS = 1\r\n        self.RATE = 16000  # 16kHz optimal for Whisper\r\n        self.RECORD_SECONDS = 5\r\n        \r\n        # Initialize PyAudio\r\n        self.p = pyaudio.PyAudio()\r\n        \r\n        # Create publisher for recognized commands\r\n        self.command_publisher = self.create_publisher(\r\n            String,\r\n            \'/robot/voice_command\',\r\n            10\r\n        )\r\n        \r\n    def record_audio(self):\r\n        """Record audio from microphone"""\r\n        stream = self.p.open(\r\n            format=self.FORMAT,\r\n            channels=self.CHANNELS,\r\n            rate=self.RATE,\r\n            input=True,\r\n            frames_per_buffer=self.CHUNK\r\n        )\r\n        \r\n        frames = []\r\n        for _ in range(0, int(self.RATE / self.CHUNK * self.RECORD_SECONDS)):\r\n            data = stream.read(self.CHUNK)\r\n            frames.append(data)\r\n        \r\n        stream.stop_stream()\r\n        stream.close()\r\n        \r\n        return b\'\'.join(frames)\r\n    \r\n    def transcribe_with_whisper(self, audio_data):\r\n        """\r\n        Transcribe audio using Whisper\r\n        \r\n        Args:\r\n            audio_data: Raw audio bytes\r\n            \r\n        Returns:\r\n            transcribed_text: String containing recognized speech\r\n        """\r\n        # Convert audio to wav format\r\n        import io\r\n        import wave\r\n        \r\n        wav_buffer = io.BytesIO()\r\n        with wave.open(wav_buffer, \'wb\') as wav_file:\r\n            wav_file.setnchannels(self.CHANNELS)\r\n            wav_file.setsampwidth(self.p.get_sample_size(self.FORMAT))\r\n            wav_file.setframerate(self.RATE)\r\n            wav_file.writeframes(audio_data)\r\n        \r\n        wav_buffer.seek(0)\r\n        \r\n        # Call OpenAI Whisper API\r\n        transcript = openai.Audio.transcribe(\r\n            model="whisper-1",\r\n            file=wav_buffer,\r\n            language="en"  # Specify language for better accuracy\r\n        )\r\n        \r\n        return transcript[\'text\']\r\n    \r\n    def listen_and_transcribe(self):\r\n        """Main loop for voice command listening"""\r\n        self.get_logger().info("\ud83c\udfa4 Listening for voice commands...")\r\n        \r\n        while rclpy.ok():\r\n            try:\r\n                # Record audio\r\n                audio_data = self.record_audio()\r\n                \r\n                # Transcribe using Whisper\r\n                command_text = self.transcribe_with_whisper(audio_data)\r\n                \r\n                self.get_logger().info(f"\ud83d\udcdd Recognized: {command_text}")\r\n                \r\n                # Publish recognized command\r\n                msg = String()\r\n                msg.data = command_text\r\n                self.command_publisher.publish(msg)\r\n                \r\n            except Exception as e:\r\n                self.get_logger().error(f"Error in voice recognition: {e}")\r\n\r\n# Entry point\r\nif __name__ == \'__main__\':\r\n    rclpy.init()\r\n    node = VoiceCommandNode()\r\n    node.listen_and_transcribe()\n'})}),"\n",(0,i.jsx)(e.h4,{id:"124-handling-edge-cases-and-robustness",children:"1.2.4 Handling Edge Cases and Robustness"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class RobustVoiceCommandProcessor:\r\n    """Handle edge cases in voice recognition"""\r\n    \r\n    def __init__(self):\r\n        self.confidence_threshold = 0.7\r\n        self.max_retries = 3\r\n        self.background_noise_threshold = -40  # dB\r\n        \r\n    def validate_audio_quality(self, audio_data):\r\n        """Check if audio quality is sufficient"""\r\n        # Compute RMS (Root Mean Square) for loudness\r\n        rms = np.sqrt(np.mean(audio_data**2))\r\n        \r\n        if rms < 0.01:\r\n            return False, "Audio too quiet"\r\n        \r\n        return True, "Audio quality OK"\r\n    \r\n    def handle_silence(self, audio_data):\r\n        """Detect and handle silence"""\r\n        threshold = np.mean(np.abs(audio_data)) * 2\r\n        silent_frames = np.sum(np.abs(audio_data) < threshold)\r\n        silence_ratio = silent_frames / len(audio_data)\r\n        \r\n        if silence_ratio > 0.8:\r\n            return "SILENCE_DETECTED"\r\n        \r\n        return None\r\n    \r\n    def retry_with_feedback(self, voice_processor):\r\n        """Retry recognition with user feedback"""\r\n        for attempt in range(self.max_retries):\r\n            try:\r\n                transcript = voice_processor.transcribe_with_whisper()\r\n                \r\n                if len(transcript) > 0:\r\n                    return transcript\r\n                    \r\n            except Exception as e:\r\n                if attempt < self.max_retries - 1:\r\n                    print(f"\u26a0\ufe0f Recognition failed. Retrying... ({attempt + 1}/{self.max_retries})")\r\n                else:\r\n                    print(f"\u274c Failed after {self.max_retries} attempts")\r\n        \r\n        return None\n'})}),"\n",(0,i.jsx)(e.h3,{id:"13-command-parsing-and-validation",children:"1.3 Command Parsing and Validation"}),"\n",(0,i.jsx)(e.p,{children:"Once speech is transcribed, the text must be parsed to extract intent and parameters."}),"\n",(0,i.jsx)(e.h4,{id:"131-intent-recognition",children:"1.3.1 Intent Recognition"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import re\r\nfrom typing import Tuple, Dict\r\n\r\nclass CommandParser:\r\n    \"\"\"Parse natural language commands into robot actions\"\"\"\r\n    \r\n    def __init__(self):\r\n        # Define command patterns\r\n        self.patterns = {\r\n            'move': r'(move|go|walk|navigate)\\s+(?:to|towards)?\\s+(\\w+)',\r\n            'pick': r'(pick up|grab|grasp)\\s+(?:the\\s+)?(\\w+)',\r\n            'place': r'(put|place|drop)\\s+(?:the\\s+)?(\\w+)\\s+(?:at|on|in)\\s+(\\w+)',\r\n            'look': r'(look|see|find|search)\\s+(?:for\\s+)?(?:the\\s+)?(\\w+)',\r\n            'stop': r'(stop|halt|pause)',\r\n            'reset': r'(reset|restart)'\r\n        }\r\n    \r\n    def parse_command(self, text: str) -> Tuple[str, Dict]:\r\n        \"\"\"\r\n        Parse command text into action and parameters\r\n        \r\n        Args:\r\n            text: Transcribed voice command\r\n            \r\n        Returns:\r\n            action: Command type (e.g., 'move', 'pick')\r\n            params: Dictionary of command parameters\r\n        \"\"\"\r\n        text = text.lower().strip()\r\n        \r\n        for action, pattern in self.patterns.items():\r\n            match = re.match(pattern, text)\r\n            if match:\r\n                groups = match.groups()\r\n                \r\n                # Extract parameters based on action\r\n                if action == 'move':\r\n                    return action, {'destination': groups[1]}\r\n                \r\n                elif action == 'pick':\r\n                    return action, {'object': groups[1]}\r\n                \r\n                elif action == 'place':\r\n                    return action, {\r\n                        'object': groups[1],\r\n                        'location': groups[2]\r\n                    }\r\n                \r\n                elif action == 'look':\r\n                    return action, {'target': groups[1]}\r\n                \r\n                elif action in ['stop', 'reset']:\r\n                    return action, {}\r\n        \r\n        return 'unknown', {}\r\n    \r\n    def validate_command(self, action: str, params: Dict) -> bool:\r\n        \"\"\"\r\n        Validate that command has all required parameters\r\n        \r\n        Args:\r\n            action: Command type\r\n            params: Command parameters\r\n            \r\n        Returns:\r\n            True if valid, False otherwise\r\n        \"\"\"\r\n        required_params = {\r\n            'move': ['destination'],\r\n            'pick': ['object'],\r\n            'place': ['object', 'location'],\r\n            'look': ['target'],\r\n            'stop': [],\r\n            'reset': []\r\n        }\r\n        \r\n        if action not in required_params:\r\n            return False\r\n        \r\n        for param in required_params[action]:\r\n            if param not in params or params[param] is None:\r\n                return False\r\n        \r\n        return True\n"})}),"\n",(0,i.jsx)(e.h4,{id:"132-confidence-scoring",children:"1.3.2 Confidence Scoring"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class ConfidenceEstimator:\r\n    \"\"\"Estimate confidence of parsed commands\"\"\"\r\n    \r\n    def estimate_confidence(self, command_text: str, \r\n                          parsed_action: str, \r\n                          parse_confidence: float) -> float:\r\n        \"\"\"\r\n        Multi-factor confidence estimation\r\n        \r\n        Args:\r\n            command_text: Original transcribed text\r\n            parsed_action: Extracted action\r\n            parse_confidence: Confidence from parser (0-1)\r\n            \r\n        Returns:\r\n            Overall confidence score (0-1)\r\n        \"\"\"\r\n        # Length-based confidence (very short = suspicious)\r\n        length_score = min(len(command_text) / 10, 1.0)\r\n        \r\n        # Known action confidence\r\n        known_actions = ['move', 'pick', 'place', 'look', 'stop', 'reset']\r\n        action_score = 1.0 if parsed_action in known_actions else 0.3\r\n        \r\n        # Combine scores\r\n        overall_confidence = (parse_confidence * 0.5 + \r\n                             length_score * 0.25 + \r\n                             action_score * 0.25)\r\n        \r\n        return overall_confidence\n"})}),"\n",(0,i.jsx)(e.hr,{})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(l,{...n})}):l(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>a,x:()=>s});var t=r(6540);const i={},o=t.createContext(i);function a(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:a(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);