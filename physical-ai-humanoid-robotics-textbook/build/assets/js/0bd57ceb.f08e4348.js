"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[379],{8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>s});var t=r(6540);const i={},o=t.createContext(i);function a(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(o.Provider,{value:n},e.children)}},8810:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Module-3-The-AI-Robot-Brain-NVIDIA-Isaac\u2122/chapter4","title":"Chapter 4: Integration and End-to-End AI-Robot System","description":"4.1 Complete System Architecture","source":"@site/docs/Module-3-The-AI-Robot-Brain-NVIDIA-Isaac\u2122/chapter4.md","sourceDirName":"Module-3-The-AI-Robot-Brain-NVIDIA-Isaac\u2122","slug":"/Module-3-The-AI-Robot-Brain-NVIDIA-Isaac\u2122/chapter4","permalink":"/docs/Module-3-The-AI-Robot-Brain-NVIDIA-Isaac\u2122/chapter4","draft":false,"unlisted":false,"editUrl":"https://github.com/<your-username>/<your-repo>/tree/main/docs/Module-3-The-AI-Robot-Brain-NVIDIA-Isaac\u2122/chapter4.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Path Planning for Bipedal Humanoid Movement","permalink":"/docs/Module-3-The-AI-Robot-Brain-NVIDIA-Isaac\u2122/chapter3"},"next":{"title":"Chapter 1: Voice-to-Action - Speech Recognition and Command Processing","permalink":"/docs/Module-4-Vision-Language-Action-VLA/chapter1"}}');var i=r(4848),o=r(8453);const a={},s="Chapter 4: Integration and End-to-End AI-Robot System",l={},c=[{value:"4.1 Complete System Architecture",id:"41-complete-system-architecture",level:3},{value:"4.1.1 System Overview",id:"411-system-overview",level:4},{value:"4.2 Training Pipeline: Sim-to-Real Transfer",id:"42-training-pipeline-sim-to-real-transfer",level:3},{value:"4.2.1 Synthetic Dataset Generation in Isaac Sim",id:"421-synthetic-dataset-generation-in-isaac-sim",level:4},{value:"4.2.2 Model Training with Synthetic Data",id:"422-model-training-with-synthetic-data",level:4},{value:"4.2.3 Validation on Real Robot",id:"423-validation-on-real-robot",level:4},{value:"4.3 Real-Time Navigation Example",id:"43-real-time-navigation-example",level:3},{value:"4.3.1 Integrated Navigation Loop",id:"431-integrated-navigation-loop",level:4},{value:"4.4 Performance Metrics and Optimization",id:"44-performance-metrics-and-optimization",level:3},{value:"4.4.1 Evaluation Metrics",id:"441-evaluation-metrics",level:4},{value:"4.4.2 Hardware Requirements",id:"442-hardware-requirements",level:4},{value:"Summary: From Simulation to Autonomous Operation",id:"summary-from-simulation-to-autonomous-operation",level:2},{value:"Development Workflow",id:"development-workflow",level:3},{value:"Key Advantages of This Integration",id:"key-advantages-of-this-integration",level:3},{value:"References and Further Reading",id:"references-and-further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-4-integration-and-end-to-end-ai-robot-system",children:"Chapter 4: Integration and End-to-End AI-Robot System"})}),"\n",(0,i.jsx)(n.h3,{id:"41-complete-system-architecture",children:"4.1 Complete System Architecture"}),"\n",(0,i.jsx)(n.h4,{id:"411-system-overview",children:"4.1.1 System Overview"}),"\n",(0,i.jsx)(n.p,{children:"The integration of Isaac Sim, Isaac ROS, and Nav2 creates a complete AI-driven autonomous humanoid system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502         ISAAC SIM (Development)                 \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\r\n\u2502  \u2502 Photorealistic Simulation + Synthetic    \u2502  \u2502\r\n\u2502  \u2502 Data Generation \u2192 AI Training Dataset    \u2502  \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n              \u2502 Trained Model\r\n              \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502         ISAAC ROS (Inference)                   \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\r\n\u2502  \u2502 Hardware-Accelerated Perception:        \u2502  \u2502\r\n\u2502  \u2502 \u2022 VSLAM for Localization & Mapping      \u2502  \u2502\r\n\u2502  \u2502 \u2022 Object Detection & Segmentation       \u2502  \u2502\r\n\u2502  \u2502 \u2022 Depth Processing                      \u2502  \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n              \u2502 Robot State & Environment\r\n              \u2193\r\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502         NAV2 (Navigation)                       \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\r\n\u2502  \u2502 Path Planning & Motion Control:         \u2502  \u2502\r\n\u2502  \u2502 \u2022 Costmap Generation                    \u2502  \u2502\r\n\u2502  \u2502 \u2022 Trajectory Planning                   \u2502  \u2502\r\n\u2502  \u2502 \u2022 Footstep Sequence Generation          \u2502  \u2502\r\n\u2502  \u2502 \u2022 Real-time Gait Control                \u2502  \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\r\n              \u2502 Motor Commands\r\n              \u2193\r\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n        \u2502  Humanoid   \u2502\r\n        \u2502   Robot     \u2502\r\n        \u2502  (Physical) \u2502\r\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h3,{id:"42-training-pipeline-sim-to-real-transfer",children:"4.2 Training Pipeline: Sim-to-Real Transfer"}),"\n",(0,i.jsx)(n.h4,{id:"421-synthetic-dataset-generation-in-isaac-sim",children:"4.2.1 Synthetic Dataset Generation in Isaac Sim"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Isaac Sim Data Generation Pipeline\r\nimport omni.isaac.sim as sim_context\r\nfrom omni.isaac.data_exporter import DataExporter\r\n\r\n# Initialize simulation\r\nsim = sim_context.SimulationContext()\r\n\r\n# Create randomization scenarios\r\nrandomizer = DomainRandomizer({\r\n    'lighting': {'intensity_range': [0.3, 1.5]},\r\n    'textures': {'variation_enabled': True},\r\n    'poses': {'position_noise': 0.05},\r\n    'materials': {'friction_range': [0.1, 1.0]}\r\n})\r\n\r\n# Generate 100,000 labeled images\r\nexporter = DataExporter()\r\nfor iteration in range(100000):\r\n    # Apply randomization\r\n    randomizer.randomize()\r\n    \r\n    # Step simulation\r\n    sim.step()\r\n    \r\n    # Export data\r\n    rgb_image = sim.get_camera_output('rgb')\r\n    depth_image = sim.get_camera_output('depth')\r\n    segmentation = sim.get_camera_output('segmentation')\r\n    annotations = sim.get_object_annotations()\r\n    \r\n    # Save with labels\r\n    exporter.save_sample({\r\n        'rgb': rgb_image,\r\n        'depth': depth_image,\r\n        'segmentation': segmentation,\r\n        'annotations': annotations\r\n    })\n"})}),"\n",(0,i.jsx)(n.h4,{id:"422-model-training-with-synthetic-data",children:"4.2.2 Model Training with Synthetic Data"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Training Deep Learning Model for Perception\r\nimport torch\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import models\r\n\r\n# Load synthetic dataset\r\nsynthetic_dataset = SyntheticDataset(\r\n    data_path='isaac_sim_exports/',\r\n    augmentation=False  # Already augmented via randomization\r\n)\r\n\r\nloader = DataLoader(synthetic_dataset, batch_size=32)\r\n\r\n# Initialize model\r\nmodel = models.resnet50(pretrained=True)\r\nmodel.fc = torch.nn.Linear(2048, 1000)  # Task-specific output layer\r\n\r\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\r\ncriterion = torch.nn.CrossEntropyLoss()\r\n\r\n# Training loop\r\nfor epoch in range(50):\r\n    for images, labels in loader:\r\n        # Forward pass\r\n        outputs = model(images)\r\n        loss = criterion(outputs, labels)\r\n        \r\n        # Backward pass\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\n"})}),"\n",(0,i.jsx)(n.h4,{id:"423-validation-on-real-robot",children:"4.2.3 Validation on Real Robot"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Deploy on Real Humanoid Robot\r\nfrom isaac_ros_core import IsaacROSNode\r\nimport rclpy\r\n\r\nclass HumanoidPerceptionNode(IsaacROSNode):\r\n    def __init__(self):\r\n        super().__init__('humanoid_perception')\r\n        \r\n        # Load trained model\r\n        self.model = torch.jit.load('model.pt')\r\n        self.model.eval()\r\n        \r\n        # Subscribe to camera topics\r\n        self.subscription = self.create_subscription(\r\n            Image,\r\n            '/camera/rgb',\r\n            self.image_callback,\r\n            10\r\n        )\r\n    \r\n    def image_callback(self, msg):\r\n        # Convert ROS image to tensor\r\n        image = self.ros_image_to_tensor(msg)\r\n        \r\n        # Inference on GPU\r\n        with torch.no_grad():\r\n            predictions = self.model(image)\r\n        \r\n        # Publish results\r\n        self.publish_predictions(predictions)\r\n\r\n# Run on Jetson\r\nrclpy.spin(HumanoidPerceptionNode())\n"})}),"\n",(0,i.jsx)(n.h3,{id:"43-real-time-navigation-example",children:"4.3 Real-Time Navigation Example"}),"\n",(0,i.jsx)(n.h4,{id:"431-integrated-navigation-loop",children:"4.3.1 Integrated Navigation Loop"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Complete navigation system for humanoid robot\r\nclass HumanoidNavigationSystem:\r\n    def __init__(self):\r\n        self.nav = BasicNavigator()\r\n        self.vslam = VisualSlamNode()\r\n        self.detector = ObjectDetectionNode()\r\n        self.costmap_updater = CostmapUpdater()\r\n        self.footstep_planner = FootstepPlanner()\r\n        \r\n    def navigate_to_goal(self, goal_pose):\r\n        """\r\n        Complete navigation pipeline:\r\n        1. Perceive environment (VSLAM + Object Detection)\r\n        2. Update costmap\r\n        3. Plan path (Nav2)\r\n        4. Generate footsteps (bipedal-specific)\r\n        5. Execute with real-time monitoring\r\n        """\r\n        \r\n        # Start navigation\r\n        self.nav.goToPose(goal_pose)\r\n        \r\n        while not self.nav.isTaskComplete():\r\n            # Get current perception\r\n            robot_pose = self.vslam.get_odometry()\r\n            environment_objects = self.detector.get_detections()\r\n            \r\n            # Update costmap with dynamic information\r\n            self.costmap_updater.update(environment_objects, robot_pose)\r\n            \r\n            # Get path from Nav2\r\n            current_path = self.nav.getPath()\r\n            \r\n            # Generate humanoid-specific footsteps\r\n            footsteps = self.footstep_planner.plan_footsteps(\r\n                robot_pose, \r\n                goal_pose, \r\n                self.costmap_updater.get_costmap()\r\n            )\r\n            \r\n            # Send footsteps to motion controller\r\n            self.send_footsteps(footsteps)\r\n            \r\n            # Check for obstacles and recovery\r\n            if self.nav.feedbackMsg is not None:\r\n                if self.is_stuck():\r\n                    self.execute_recovery_behavior()\r\n            \r\n            # Small delay for loop rate control\r\n            time.sleep(0.1)\r\n        \r\n        print("Goal reached!")\r\n    \r\n    def send_footsteps(self, footsteps):\r\n        """Send footstep commands to locomotion controller"""\r\n        for step in footsteps:\r\n            # Convert to motor commands\r\n            joint_targets = self.ik_solver.solve(step)\r\n            \r\n            # Execute with trajectory tracking\r\n            self.trajectory_executor.execute(joint_targets)\r\n    \r\n    def is_stuck(self):\r\n        """Detect if robot is stuck in obstacle"""\r\n        feedback = self.nav.feedbackMsg\r\n        progress_threshold = 0.1  # meters\r\n        time_window = 5.0  # seconds\r\n        \r\n        if (time.time() - self.last_position_update) > time_window:\r\n            if np.linalg.norm(feedback.current_pose - self.last_pose) < progress_threshold:\r\n                return True\r\n        \r\n        self.last_pose = feedback.current_pose\r\n        self.last_position_update = time.time()\r\n        return False\r\n    \r\n    def execute_recovery_behavior(self):\r\n        """Attempt to recover from stuck state"""\r\n        print("Robot stuck, executing recovery...")\r\n        # Try backing up\r\n        self.footstep_planner.generate_backup_steps(num_steps=3)\r\n        # Try alternate path\r\n        self.nav.cancelTask()\r\n        time.sleep(1.0)\r\n        self.navigate_to_goal(self.goal_pose)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"44-performance-metrics-and-optimization",children:"4.4 Performance Metrics and Optimization"}),"\n",(0,i.jsx)(n.h4,{id:"441-evaluation-metrics",children:"4.4.1 Evaluation Metrics"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Metric"}),(0,i.jsx)(n.th,{children:"Unit"}),(0,i.jsx)(n.th,{children:"Target"}),(0,i.jsx)(n.th,{children:"Method"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Localization Error"})}),(0,i.jsx)(n.td,{children:"cm"}),(0,i.jsx)(n.td,{children:"< 5"}),(0,i.jsx)(n.td,{children:"Compare VSLAM pose vs ground truth"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Map Accuracy"})}),(0,i.jsx)(n.td,{children:"%"}),(0,i.jsx)(n.td,{children:"> 95"}),(0,i.jsx)(n.td,{children:"Compute map-to-real correspondence"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Path Planning Time"})}),(0,i.jsx)(n.td,{children:"ms"}),(0,i.jsx)(n.td,{children:"< 100"}),(0,i.jsx)(n.td,{children:"Measure planning algorithm latency"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Navigation Success Rate"})}),(0,i.jsx)(n.td,{children:"%"}),(0,i.jsx)(n.td,{children:"> 95"}),(0,i.jsx)(n.td,{children:"Test multiple goal locations"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Power Consumption"})}),(0,i.jsx)(n.td,{children:"W"}),(0,i.jsx)(n.td,{children:"< 500"}),(0,i.jsx)(n.td,{children:"Monitor battery during operation"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Real-time Factor"})}),(0,i.jsx)(n.td,{children:"-"}),(0,i.jsx)(n.td,{children:"> 1.0"}),(0,i.jsx)(n.td,{children:"Verify 30 fps+ perception"})]})]})]}),"\n",(0,i.jsx)(n.h4,{id:"442-hardware-requirements",children:"4.4.2 Hardware Requirements"}),"\n",(0,i.jsx)(n.p,{children:"For optimal performance of the integrated system:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Development Machine (Isaac Sim):"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"GPU: NVIDIA RTX 4090 or A6000 (recommended)"}),"\n",(0,i.jsx)(n.li,{children:"CPU: Intel Xeon / AMD EPYC (12+ cores)"}),"\n",(0,i.jsx)(n.li,{children:"RAM: 64 GB"}),"\n",(0,i.jsx)(n.li,{children:"SSD: 500 GB NVMe"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Edge Device (Isaac ROS):"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"SoC: NVIDIA Jetson Orin (15-275 TOPS AI performance)"}),"\n",(0,i.jsx)(n.li,{children:"RAM: 12-32 GB"}),"\n",(0,i.jsx)(n.li,{children:"Storage: 256 GB SSD"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Robot Compute:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Main Controller: ARM-based processor (on-board)"}),"\n",(0,i.jsx)(n.li,{children:"Servo Controllers: Distributed microcontrollers"}),"\n",(0,i.jsx)(n.li,{children:"Power: 10-50 kW battery (varies by robot size)"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"summary-from-simulation-to-autonomous-operation",children:"Summary: From Simulation to Autonomous Operation"}),"\n",(0,i.jsx)(n.h3,{id:"development-workflow",children:"Development Workflow"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Design Phase"}),": Create robot model in Isaac Sim"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Simulation Phase"}),": Generate synthetic training data with domain randomization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Training Phase"}),": Train perception models on GPU"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Development Phase"}),": Test Isaac ROS modules on edge hardware"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Navigation Phase"}),": Integrate Nav2 for autonomous movement"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Testing Phase"}),": Validate on simulated humanoid"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Deployment Phase"}),": Transfer to real robot with fine-tuning"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"key-advantages-of-this-integration",children:"Key Advantages of This Integration"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Photorealism \u2192 Better Performance"}),": Synthetic data from Isaac Sim directly improves real-world perception"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hardware Acceleration"}),": Isaac ROS VSLAM and object detection run at real-time speeds on edge devices"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Flexible Navigation"}),": Nav2 adapted for bipedal motion enables natural humanoid movement"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scalability"}),": System can handle complex multi-robot scenarios"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Continuous Improvement"}),": New data and training iterations improve performance iteratively"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"references-and-further-reading",children:"References and Further Reading"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["NVIDIA Isaac Sim Documentation: ",(0,i.jsx)(n.a,{href:"https://docs.nvidia.com/isaac/isaac-sim/",children:"https://docs.nvidia.com/isaac/isaac-sim/"})]}),"\n",(0,i.jsxs)(n.li,{children:["Isaac ROS GitHub Repository: ",(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA-ISAAC-ROS",children:"https://github.com/NVIDIA-ISAAC-ROS"})]}),"\n",(0,i.jsxs)(n.li,{children:["Nav2 Documentation: ",(0,i.jsx)(n.a,{href:"https://navigation.ros.org",children:"https://navigation.ros.org"})]}),"\n",(0,i.jsxs)(n.li,{children:["ROS 2 Official Documentation: ",(0,i.jsx)(n.a,{href:"https://docs.ros.org/en/humble/",children:"https://docs.ros.org/en/humble/"})]}),"\n",(0,i.jsx)(n.li,{children:"Photorealistic Rendering in Robotics: Domain Randomization Techniques"}),"\n",(0,i.jsx)(n.li,{children:"Sim-to-Real Transfer Learning: Best Practices"}),"\n",(0,i.jsx)(n.li,{children:"Bipedal Robot Control and Gait Planning Literature"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);