"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[67],{7883:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>i,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"Module-4-Vision-Language-Action-VLA/chapter4","title":"Chapter 4: Capstone Project - The Autonomous Humanoid","description":"4.1 Project Overview","source":"@site/docs/Module-4-Vision-Language-Action-VLA/chapter4.md","sourceDirName":"Module-4-Vision-Language-Action-VLA","slug":"/Module-4-Vision-Language-Action-VLA/chapter4","permalink":"/docs/Module-4-Vision-Language-Action-VLA/chapter4","draft":false,"unlisted":false,"editUrl":"https://github.com/<your-username>/<your-repo>/tree/main/docs/Module-4-Vision-Language-Action-VLA/chapter4.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: From Planning to Execution - ROS 2 Action Integration","permalink":"/docs/Module-4-Vision-Language-Action-VLA/chapter3"}}');var o=r(4848),s=r(8453);const i={},a="Chapter 4: Capstone Project - The Autonomous Humanoid",c={},l=[{value:"4.1 Project Overview",id:"41-project-overview",level:3},{value:"4.2 System Architecture",id:"42-system-architecture",level:3},{value:"4.3 Capstone Project Implementation",id:"43-capstone-project-implementation",level:3},{value:"4.3.1 Main Application Loop",id:"431-main-application-loop",level:4},{value:"4.3.2 Testing Scenarios",id:"432-testing-scenarios",level:4},{value:"4.4 Performance Metrics and Evaluation",id:"44-performance-metrics-and-evaluation",level:3},{value:"4.5 Deployment Checklist",id:"45-deployment-checklist",level:3},{value:"Integration Summary",id:"integration-summary",level:2},{value:"Key Innovations in This Module",id:"key-innovations-in-this-module",level:3},{value:"References and Further Reading",id:"references-and-further-reading",level:2},{value:"Capstone Project Submission Guidelines",id:"capstone-project-submission-guidelines",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-4-capstone-project---the-autonomous-humanoid",children:"Chapter 4: Capstone Project - The Autonomous Humanoid"})}),"\n",(0,o.jsx)(n.h3,{id:"41-project-overview",children:"4.1 Project Overview"}),"\n",(0,o.jsx)(n.p,{children:"The capstone project integrates all components into a complete autonomous humanoid robot system capable of:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Understanding"})," spoken commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Planning"})," complex multi-step tasks"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigating"})," dynamic environments with obstacles"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perceiving"})," objects using computer vision"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulating"})," objects with arms and hands"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"42-system-architecture",children:"4.2 System Architecture"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                  HUMANOID ROBOT                        \u2502\r\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\r\n\u2502                                                        \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\r\n\u2502  \u2502         Voice Command Interface                 \u2502 \u2502\r\n\u2502  \u2502  (Whisper ASR + Command Parser)                 \u2502 \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\r\n\u2502                        \u2502 Command                     \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2193\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\r\n\u2502  \u2502         Cognitive Planning Layer               \u2502 \u2502\r\n\u2502  \u2502  (LLM: GPT-4 / Claude)                        \u2502 \u2502\r\n\u2502  \u2502  Task decomposition \u2192 Action sequence          \u2502 \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\r\n\u2502                        \u2502 Plan                        \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2193\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\r\n\u2502  \u2502    Perception and Navigation Layer             \u2502 \u2502\r\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502 \u2502\r\n\u2502  \u2502  \u2502 VSLAM/Odom  \u2502  \u2502  Nav2 Path  \u2502             \u2502 \u2502\r\n\u2502  \u2502  \u2502             \u2502  \u2502  Planning   \u2502             \u2502 \u2502\r\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502 \u2502\r\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502 \u2502\r\n\u2502  \u2502  \u2502 Object      \u2502  \u2502 Depth/      \u2502             \u2502 \u2502\r\n\u2502  \u2502  \u2502 Detection   \u2502  \u2502 Segmentation\u2502             \u2502 \u2502\r\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502 \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\r\n\u2502                        \u2502 State update               \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2193\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\r\n\u2502  \u2502         Motion Control Layer                   \u2502 \u2502\r\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502 \u2502\r\n\u2502  \u2502  \u2502 Locomotion   \u2502  \u2502 Manipulation \u2502           \u2502 \u2502\r\n\u2502  \u2502  \u2502 (Footsteps)  \u2502  \u2502 (Arms/Hands) \u2502           \u2502 \u2502\r\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502 \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\r\n\u2502                        \u2502 Actuator commands         \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2193\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\r\n\u2502  \u2502         Hardware Interface                     \u2502 \u2502\r\n\u2502  \u2502  (Joint Controllers, Gripper, Sensors)        \u2502 \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(n.h3,{id:"43-capstone-project-implementation",children:"4.3 Capstone Project Implementation"}),"\n",(0,o.jsx)(n.h4,{id:"431-main-application-loop",children:"4.3.1 Main Application Loop"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'"""\r\nAutonomous Humanoid Robot - Capstone Project\r\nDemonstrates: Voice \u2192 Plan \u2192 Navigate \u2192 Detect \u2192 Manipulate\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.executors import MultiThreadedExecutor\r\nimport json\r\nimport time\r\n\r\nclass AutonomousHumanoid(Node):\r\n    """Main capstone system coordinator"""\r\n    \r\n    def __init__(self):\r\n        super().__init__(\'autonomous_humanoid\')\r\n        \r\n        # Initialize components\r\n        self.voice_node = VoiceCommandNode()\r\n        self.planner = CognitivePlanner()\r\n        self.executor = PlanExecutor()\r\n        self.vision_system = VisionSystem()\r\n        self.navigator = NavigationController()\r\n        self.manipulator = ManipulationController()\r\n        \r\n        # Create multithreaded executor for concurrent operation\r\n        self.executor_threads = MultiThreadedExecutor(num_threads=4)\r\n        \r\n        # State tracking\r\n        self.robot_state = {\r\n            \'position\': [0.0, 0.0],\r\n            \'orientation\': 0.0,\r\n            \'battery\': 100.0,\r\n            \'gripper_state\': \'open\',\r\n            \'task_status\': \'idle\',\r\n            \'current_plan\': []\r\n        }\r\n        \r\n        self.get_logger().info("\ud83e\udd16 Autonomous Humanoid Robot Initialized")\r\n        self.get_logger().info("Waiting for voice commands... Say \'Clean the room\' or \'Pick up the book\'")\r\n    \r\n    def run(self):\r\n        """Main execution loop"""\r\n        \r\n        while rclpy.ok():\r\n            try:\r\n                # Check for voice commands\r\n                if self.voice_node.has_new_command():\r\n                    command = self.voice_node.get_command()\r\n                    self.get_logger().info(f"\ud83c\udfa4 Received: {command}")\r\n                    \r\n                    # Process command\r\n                    self.process_task_command(command)\r\n                \r\n                # Update robot state periodically\r\n                self.update_robot_state()\r\n                \r\n                time.sleep(0.1)\r\n            \r\n            except Exception as e:\r\n                self.get_logger().error(f"Error in main loop: {e}")\r\n    \r\n    def process_task_command(self, command: str):\r\n        """\r\n        Complete pipeline: Parse \u2192 Plan \u2192 Execute\r\n        """\r\n        \r\n        self.robot_state[\'task_status\'] = \'planning\'\r\n        \r\n        # Step 1: Generate plan using LLM\r\n        self.get_logger().info("\ud83e\udde0 Generating cognitive plan...")\r\n        plan = self.planner.plan_task(command)\r\n        \r\n        if not plan:\r\n            self.get_logger().error("Failed to generate plan")\r\n            self.robot_state[\'task_status\'] = \'failed\'\r\n            return\r\n        \r\n        self.robot_state[\'current_plan\'] = plan\r\n        \r\n        # Print plan for debugging\r\n        self.get_logger().info(f"\ud83d\udccb Generated plan with {len(plan)} steps:")\r\n        for i, step in enumerate(plan, 1):\r\n            self.get_logger().info(\r\n                f"  {i}. {step[\'action\']}({json.dumps(step[\'parameters\'])})"\r\n            )\r\n        \r\n        # Step 2: Execute plan\r\n        self.robot_state[\'task_status\'] = \'executing\'\r\n        self.get_logger().info("\u26a1 Executing plan...")\r\n        \r\n        success = self.execute_full_plan(plan)\r\n        \r\n        if success:\r\n            self.get_logger().info("\u2705 Task completed successfully!")\r\n            self.robot_state[\'task_status\'] = \'idle\'\r\n        else:\r\n            self.get_logger().error("\u274c Task execution failed")\r\n            self.robot_state[\'task_status\'] = \'failed\'\r\n    \r\n    def execute_full_plan(self, plan: List[Dict]) -> bool:\r\n        """Execute complete plan with all subsystems"""\r\n        \r\n        for step_idx, step in enumerate(plan):\r\n            action = step[\'action\']\r\n            params = step[\'parameters\']\r\n            \r\n            self.get_logger().info(\r\n                f"Step {step_idx + 1}: {action} {params}"\r\n            )\r\n            \r\n            try:\r\n                if action == \'navigate\':\r\n                    success = self.navigate_to_location(params[\'destination\'])\r\n                \r\n                elif action == \'look_for\':\r\n                    success = self.detect_object(params[\'target\'])\r\n                \r\n                elif action == \'pick_up\':\r\n                    success = self.pick_up_object(params[\'object\'])\r\n                \r\n                elif action == \'place\':\r\n                    success = self.place_object(\r\n                        params[\'object\'],\r\n                        params[\'location\']\r\n                    )\r\n                \r\n                elif action == \'open\':\r\n                    success = self.open_door(params[\'target\'])\r\n                \r\n                elif action == \'close\':\r\n                    success = self.close_door(params[\'target\'])\r\n                \r\n                else:\r\n                    self.get_logger().warn(f"Unknown action: {action}")\r\n                    success = False\r\n                \r\n                if not success:\r\n                    self.get_logger().warn(f"Step {step_idx + 1} failed, attempting recovery...")\r\n                    # Could implement recovery here\r\n                    return False\r\n            \r\n            except Exception as e:\r\n                self.get_logger().error(f"Exception in step {step_idx}: {e}")\r\n                return False\r\n        \r\n        return True\r\n    \r\n    def navigate_to_location(self, location: str) -> bool:\r\n        """Navigate to named location"""\r\n        \r\n        self.get_logger().info(f"\ud83d\udeb6 Navigating to {location}...")\r\n        \r\n        # Get coordinates\r\n        coords = self.navigator.location_to_coordinates(location)\r\n        if coords is None:\r\n            self.get_logger().error(f"Unknown location: {location}")\r\n            return False\r\n        \r\n        # Start navigation\r\n        success = self.navigator.navigate_to(coords)\r\n        \r\n        if success:\r\n            self.robot_state[\'position\'] = list(coords)\r\n            self.get_logger().info(f"\u2713 Reached {location}")\r\n        \r\n        return success\r\n    \r\n    def detect_object(self, object_name: str) -> bool:\r\n        """Use computer vision to detect object"""\r\n        \r\n        self.get_logger().info(f"\ud83d\udc41\ufe0f  Looking for {object_name}...")\r\n        \r\n        # Get camera frame\r\n        frame = self.vision_system.get_frame()\r\n        \r\n        # Run object detection\r\n        detections = self.vision_system.detect_objects(frame)\r\n        \r\n        # Search for target object\r\n        for detection in detections:\r\n            if self.vision_system.match_label(detection[\'label\'], object_name):\r\n                self.get_logger().info(f"\u2713 Found {object_name}")\r\n                return True\r\n        \r\n        self.get_logger().warn(f"\u2717 {object_name} not found in view")\r\n        return False\r\n    \r\n    def pick_up_object(self, obj: str) -> bool:\r\n        """Pick up object"""\r\n        \r\n        self.get_logger().info(f"\ud83e\udd32 Picking up {obj}...")\r\n        \r\n        # Use vision to locate object\r\n        pose = self.vision_system.get_object_pose(obj)\r\n        \r\n        if pose is None:\r\n            self.get_logger().warn(f"Cannot locate {obj}")\r\n            return False\r\n        \r\n        # Move arm to object\r\n        success = self.manipulator.reach_to_pose(pose)\r\n        \r\n        if not success:\r\n            return False\r\n        \r\n        # Close gripper\r\n        success = self.manipulator.gripper_close()\r\n        \r\n        if success:\r\n            self.robot_state[\'gripper_state\'] = \'closed\'\r\n            self.get_logger().info(f"\u2713 Picked up {obj}")\r\n        \r\n        return success\r\n    \r\n    def place_object(self, obj: str, location: str) -> bool:\r\n        """Place object at location"""\r\n        \r\n        self.get_logger().info(f"\ud83d\udce6 Placing {obj} at {location}...")\r\n        \r\n        # Get placement coordinates\r\n        coords = self.navigator.location_to_coordinates(location)\r\n        if coords is None:\r\n            return False\r\n        \r\n        # Navigate to location\r\n        success = self.navigate_to_location(location)\r\n        if not success:\r\n            return False\r\n        \r\n        # Compute arm pose for placement\r\n        placement_pose = self.manipulator.compute_placement_pose(coords)\r\n        \r\n        # Move arm to placement location\r\n        success = self.manipulator.reach_to_pose(placement_pose)\r\n        if not success:\r\n            return False\r\n        \r\n        # Open gripper\r\n        success = self.manipulator.gripper_open()\r\n        \r\n        if success:\r\n            self.robot_state[\'gripper_state\'] = \'open\'\r\n            self.get_logger().info(f"\u2713 Placed {obj} at {location}")\r\n        \r\n        return success\r\n    \r\n    def open_door(self, door: str) -> bool:\r\n        """Open door or drawer"""\r\n        \r\n        self.get_logger().info(f"\ud83d\udeaa Opening {door}...")\r\n        \r\n        # Locate door handle\r\n        handle_pose = self.vision_system.get_handle_pose(door)\r\n        \r\n        if handle_pose is None:\r\n            return False\r\n        \r\n        # Execute opening motion\r\n        success = self.manipulator.execute_open_motion(handle_pose)\r\n        \r\n        return success\r\n    \r\n    def close_door(self, door: str) -> bool:\r\n        """Close door or drawer"""\r\n        \r\n        self.get_logger().info(f"\ud83d\udeaa Closing {door}...")\r\n        \r\n        # Execute closing motion\r\n        success = self.manipulator.execute_close_motion(door)\r\n        \r\n        return success\r\n    \r\n    def update_robot_state(self):\r\n        """Periodically update robot state"""\r\n        \r\n        # Update odometry\r\n        odometry = self.navigator.get_odometry()\r\n        if odometry:\r\n            self.robot_state[\'position\'] = [odometry.x, odometry.y]\r\n            self.robot_state[\'orientation\'] = odometry.theta\r\n        \r\n        # Update battery\r\n        battery_percent = self.get_battery_level()\r\n        self.robot_state[\'battery\'] = battery_percent\r\n        \r\n        if battery_percent < 20:\r\n            self.get_logger().warn("\u26a0\ufe0f Battery low")\n'})}),"\n",(0,o.jsx)(n.h4,{id:"432-testing-scenarios",children:"4.3.2 Testing Scenarios"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class CapstoneTestSuite:\r\n    \"\"\"Test scenarios for capstone project\"\"\"\r\n    \r\n    TEST_SCENARIOS = [\r\n        {\r\n            'name': 'Simple Navigation',\r\n            'command': 'Go to the kitchen',\r\n            'expected_steps': ['navigate']\r\n        },\r\n        {\r\n            'name': 'Object Detection',\r\n            'command': 'Find the book',\r\n            'expected_steps': ['look_for']\r\n        },\r\n        {\r\n            'name': 'Pick and Place',\r\n            'command': 'Pick up the cup and place it on the table',\r\n            'expected_steps': ['look_for', 'pick_up', 'navigate', 'place']\r\n        },\r\n        {\r\n            'name': 'Complex Task',\r\n            'command': 'Clean the room',\r\n            'expected_steps': ['navigate', 'look_for', 'pick_up', 'place']\r\n        },\r\n        {\r\n            'name': 'Interactive Task',\r\n            'command': 'Open the door and go outside',\r\n            'expected_steps': ['open', 'navigate']\r\n        }\r\n    ]\r\n    \r\n    @staticmethod\r\n    def run_all_tests():\r\n        \"\"\"Execute all test scenarios\"\"\"\r\n        \r\n        humanoid = AutonomousHumanoid()\r\n        results = []\r\n        \r\n        for scenario in CapstoneTestSuite.TEST_SCENARIOS:\r\n            result = CapstoneTestSuite.run_test(humanoid, scenario)\r\n            results.append(result)\r\n            \r\n            # Print result\r\n            status = \"\u2705 PASS\" if result['success'] else \"\u274c FAIL\"\r\n            print(f\"{status}: {scenario['name']}\")\r\n        \r\n        # Summary\r\n        passed = sum(1 for r in results if r['success'])\r\n        total = len(results)\r\n        print(f\"\\n\ud83d\udcca Results: {passed}/{total} scenarios passed\")\r\n        \r\n        return results\r\n    \r\n    @staticmethod\r\n    def run_test(humanoid: AutonomousHumanoid, scenario: Dict) -> Dict:\r\n        \"\"\"Run single test scenario\"\"\"\r\n        \r\n        start_time = time.time()\r\n        \r\n        try:\r\n            # Execute scenario\r\n            humanoid.process_task_command(scenario['command'])\r\n            \r\n            elapsed = time.time() - start_time\r\n            \r\n            return {\r\n                'scenario': scenario['name'],\r\n                'success': True,\r\n                'time': elapsed,\r\n                'error': None\r\n            }\r\n        \r\n        except Exception as e:\r\n            return {\r\n                'scenario': scenario['name'],\r\n                'success': False,\r\n                'time': time.time() - start_time,\r\n                'error': str(e)\r\n            }\n"})}),"\n",(0,o.jsx)(n.h3,{id:"44-performance-metrics-and-evaluation",children:"4.4 Performance Metrics and Evaluation"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Metric"}),(0,o.jsx)(n.th,{children:"Target"}),(0,o.jsx)(n.th,{children:"Measurement"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Plan Generation Time"})}),(0,o.jsx)(n.td,{children:"< 2 seconds"}),(0,o.jsx)(n.td,{children:"LLM inference latency"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Command Recognition Accuracy"})}),(0,o.jsx)(n.td,{children:"> 95%"}),(0,o.jsx)(n.td,{children:"Whisper WER (Word Error Rate)"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Plan Execution Success Rate"})}),(0,o.jsx)(n.td,{children:"> 90%"}),(0,o.jsx)(n.td,{children:"Number of successful completions"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Navigation Accuracy"})}),(0,o.jsx)(n.td,{children:"< 10 cm"}),(0,o.jsx)(n.td,{children:"VSLAM pose error"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"Object Detection Accuracy"})}),(0,o.jsx)(n.td,{children:"> 85%"}),(0,o.jsx)(n.td,{children:"mAP (mean Average Precision)"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"End-to-End Task Time"})}),(0,o.jsx)(n.td,{children:"< 5 min"}),(0,o.jsx)(n.td,{children:"Total time from command to completion"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"System Responsiveness"})}),(0,o.jsx)(n.td,{children:"< 500 ms"}),(0,o.jsx)(n.td,{children:"Latency from command to action start"})]})]})]}),"\n",(0,o.jsx)(n.h3,{id:"45-deployment-checklist",children:"4.5 Deployment Checklist"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-markdown",children:"## Pre-Deployment Verification\r\n\r\n- [ ] Voice recognition working with acceptable WER\r\n- [ ] LLM planning generating valid task sequences\r\n- [ ] Navigation system reliable in test environment\r\n- [ ] Object detection accurate for target objects\r\n- [ ] Manipulation safe and reliable\r\n- [ ] Safety stops functioning\r\n- [ ] Real-time performance (30+ Hz perception)\r\n- [ ] Battery management and charging working\r\n- [ ] Logging and monitoring active\r\n- [ ] Human-robot interaction protocols established\r\n- [ ] Emergency procedures tested\r\n- [ ] Documentation complete\r\n\r\n## Safety Requirements\r\n\r\n- Emergency stop button accessible\r\n- Robot never approaches humans closer than safety distance\r\n- Gripper force limited to safe levels\r\n- Navigation avoids collision with obstacles\r\n- Continuous monitoring of system health\n"})}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"integration-summary",children:"Integration Summary"}),"\n",(0,o.jsx)(n.p,{children:"The Vision-Language-Action (VLA) framework represents the culmination of modern AI and robotics:"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Voice Understanding"})," \u2192 ",(0,o.jsx)(n.strong,{children:"Cognitive Planning"})," \u2192 ",(0,o.jsx)(n.strong,{children:"Perception & Navigation"})," \u2192 ",(0,o.jsx)(n.strong,{children:"Manipulation"})]}),"\n",(0,o.jsx)(n.p,{children:"This pipeline enables humanoid robots to engage in natural conversation and autonomously execute complex tasks in real-world environments."}),"\n",(0,o.jsx)(n.h3,{id:"key-innovations-in-this-module",children:"Key Innovations in This Module"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multimodal AI"}),": Combining speech, language, and vision systems"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Reasoning Under Uncertainty"}),": LLMs handle ambiguity in commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Adaptive Execution"}),": Dynamic replanning based on environmental feedback"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Human-Robot Collaboration"}),": Intuitive interfaces for non-technical users"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"End-to-End Learning"}),": Sim-to-real transfer validated through capstone project"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"references-and-further-reading",children:"References and Further Reading"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"OpenAI Whisper: Robust Speech Recognition via Large-Scale Weak Supervision"}),"\n",(0,o.jsx)(n.li,{children:"GPT-4 Technical Report and API Documentation"}),"\n",(0,o.jsx)(n.li,{children:"ROS 2 Action Servers and Clients Guide"}),"\n",(0,o.jsx)(n.li,{children:"VSLAM and Visual Localization State-of-the-art"}),"\n",(0,o.jsx)(n.li,{children:"Humanoid Robot Control and Gait Planning"}),"\n",(0,o.jsx)(n.li,{children:"Human-Robot Interaction Best Practices"}),"\n",(0,o.jsx)(n.li,{children:"Safety Standards for Collaborative Robots (ISO/TS 15066)"}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"capstone-project-submission-guidelines",children:"Capstone Project Submission Guidelines"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Deliverables:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Source code (GitHub repository)"}),"\n",(0,o.jsx)(n.li,{children:"System architecture documentation"}),"\n",(0,o.jsx)(n.li,{children:"Test results and performance metrics"}),"\n",(0,o.jsx)(n.li,{children:"Video demonstrations (minimum 5 scenarios)"}),"\n",(0,o.jsx)(n.li,{children:"Final report (5-10 pages)"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"Evaluation Criteria:"})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Functionality (Plan generation, execution reliability)"}),"\n",(0,o.jsx)(n.li,{children:"Innovation (Novel use of LLMs or perception systems)"}),"\n",(0,o.jsx)(n.li,{children:"Robustness (Error handling, recovery mechanisms)"}),"\n",(0,o.jsx)(n.li,{children:"Code quality (Documentation, modularity)"}),"\n",(0,o.jsx)(n.li,{children:"Performance (Speed, accuracy metrics)"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>a});var t=r(6540);const o={},s=t.createContext(o);function i(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);